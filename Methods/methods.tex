\chapter{Methods}
\section{Architecture of VEGA}\label{methods:model}
For simplicity, we follow the same notation as Seninge et al. (2021). VEGA is an interpretable VAE consisting of a two-layer nonlinear encoder (inference part) and a single-layer linear decoder (generative part), which attempts to maximize the likelihood of a single-cell dataset $X$ under a generative process\cite{Seninge2021,Kingma2014,Lotfollahi2019}, formulated as:
\begin{equation}
    p(X\,|\,\Theta) = \int p(X\,|\,Z,\Theta)p(Z\,|\,\Theta)dZ
\end{equation}
where $\Theta$ denotes the learnable parameters of the model and $Z$ represents the latent variables. Since the decoder is single-layer and linear, predefined gene modules (gene sets), such as pathways, GRNs and so on, can be used to initiate the decoder wiring, which can then model a set of latent variables $Z$ as certain biological entities. To be more concrete, the connections between a latent variable $z^{(j)}$ and the gene features in the output layer can be specified using a binary mask $M$ where $M_{i,j} = 1$ (true) if a gene $i$ from the feature list is annotated in the gene module $j$ and $M_{i,j} = 0$ (false) otherwise and those connections which are masked off ($M_{i,j} = 0$) will always have zero-valued weights during model training, constraining each latent variable to connect to a certain subset of genes in the decoder (a column of the mask matrix $M$). Of note, each latent variable $z^{(j)}$ can be referred to as a gene module variable (GMV) because each represents the corresponding gene modules $j$, which enables us to interpret a single-cell dataset $X$ from another viewpoint. Besides, the weights in the decoder are constrained to be non-negative to maintain the interpretability of the latent space. Since the gene reconstructions in the decoder are linear transformations and the GMVs can be both positive and negative, predictions in the latent space can be totally opposite if the weights are not constrained to be non-negative.

To have the model generative, the GMVs are modeled as the posterior distribution given single-cell data $X$ with a variational distribution through an inference process in the encoder, which makes the latent space more continuous and complete\cite{Rocca2019}. However, since the posterior distribution $p(Z\,|\,X)$ is usually intractable, another simpler and more tractable distribution $q(Z\,|\,X)$ is used to simplify the problem. Modeling the latent space as a multivariate normal distribution is a common choice and has been demonstrated to work well in previous single-cell transcriptome studies\cite{Seninge2021,Lopez2018,Lotfollahi2019}, which is formulated as:
\begin{equation}
    q(Z\,|\,X,\phi) = \mathcal{N}(\mu_\phi(X),\,\Sigma_\phi(X))
\end{equation}
where $\phi$ denotes the learnable parameters of the encoder. As a standard VAE implementation\cite{Kingma2014}, the objective is to maximize the evidence lower bound (ELBO) during model training, described as:
\begin{equation}
    \mathcal{L}(X) = E_{q(Z\,|\,X,\phi)}[\,\textnormal{log}\,p(X\,|\,Z,\Theta)\,]\,-\,\textnormal{KLD}(q(Z\,|\,X,\phi)\,||\,p(Z\,|\,\Theta))
\end{equation}
where the expected value of the variational distribution can be approximated using Monte Carlo integration\cite{Cumer2020} and the Kullback-Leibler divergence\cite{Joyce2011} (KLD) term has a closed-form solution because the prior $p(Z\,|\,\Theta)$ is set to $\mathcal{N}(0,1)$. Since the GMVs are sampled from the latent normal distribution, the gradients fail to flow through fully stochastic nodes during backpropagation\cite{Insights2018}. For this end, the reparameterization trick\cite{Kingma2014} is used to enable effective model training.

There are two novel hyperparameters introduced in VEGA: A dropout layer and additional fully connected nodes in the latent space. A nature of each node in a deep learning model is to learn most important information in data as diverse as possible and the learned information is randomly distributed over the nodes. To this end, if there are two gene modules in prior knowledge highly overlapping with each other, the wiring of these two GMVs in the decoder will be very similar, so instead of modeling both the latent variables as certain highly correlated gene modules, the model may be forced to learn only one arbitrary gene module or randomly share the learned information between the latent variables, leading to loss of information and interpretability. To address this issue, a dropout layer is employed in the latent space, which has been demonstrated to help the model preserve highly correlated gene modules\cite{Seninge2021}.

Since predefined gene modules which we use as prior knowledge cannot always cover all gene features in the output layer, i.e., there are no connections between the GMVs and the genes which are not annotated in any predefined gene modules, the reconstructions of those unannotated genes during model training will be problematic. Besides, some of those unannotated genes might provide undiscovered meaningful biological information, which can make the interpretable latent space even more informative. To this end, additional fully connected nodes are employed in the latent space to (1) enable the model to reconstruct those unannotated genes during training, which can boost predictive performance on gene expressioin values, and (2) help the model capture possible additional important information that is unexplained in the prior. However, notice that the number of additional fully connected nodes used in the latent space is a trade-off between the model predictive performance and the loss of information in the GMVs. In principle, increasing additional fully connected nodes results in better predictive performance but less informative GMVs.

Last but not least, incorporating batch information through one-hot encoding in the encoder and the latent space has been proved to be able to alleviate batch effects\cite{Seninge2021,Lopez2018}. For doing so, categorical covariates are directly concatenated to input data (additional nodes in the input layer) and representations of the input data (additional nodes in the latent space).

\section{Implementation of L1 regularization technique}\label{methods:L1}
The L1 regularization technique\cite{Ng2004} adds a penalty term to the loss function, which encourages the sum of the absolute values of the model parameters to be as small as possible during model training. This is effective in preventing overfitting because the weights of less important features will be gradually shrunk to zero, resulting in sparse feature vectors. The loss function with the L1 regularization term is formulated as:
\begin{equation}
    L + \lambda\sum^{n}_{j=1}|\theta_j|
\end{equation}
where $L$ represents the loss function of a model, $\lambda$ denotes the regularization parameter and $\theta_j$ denotes a model parameter. When $\lambda$ is set to a very large value, it will make more weights become zero during model training, leading to underfitting. On the other hand, when $\lambda$ is too small, the effect of model regularization will be unnoticeable.

To enable the single-layer linear decoder of the an interpretable VAE to use prior knowledge more flexibly, instead of hard coding the decoder wiring\cite{Seninge2021}, the L1 regularization technique can be selectively employed on weights in the decoder through a binary mask, introduced in Rybakov et al. (2020). Following the same notation used in Methods \ref{methods:model}, prior gene modules are converted into a binary mask $M$ where $M_{i,j} = 1$ if a gene $i$ from the feature list is not annotated in a gene module $j$ and $M_{i,j} = 0$ otherwise. Note that a binary mask based on the prior used in the regularized decoder is opposite to that used in the hard-coded decoder, which pinpoints the unannotated relationships between GMVs and genes, where the L1 regularization will be imposed on. The loss function of the model is formulated as:
\begin{equation}
    L + \lambda\sum^{n}_{j=1}\|W_{:,j}\circ M_{:,j}\|_1
\end{equation}
where $W_{:,j}\circ M_{:,j}$ represents the element-wise product between the j-th column of the decoder weight matrix $W$ and the j-th column of the binary mask $M$, which means that $\|W_{:,j}\circ M_{:,j}\|_1$ is the sum of the absolute values of weights for feature genes that are not annotated in a gene module $j$. The weights of those decoder connections which are not annotated in prior knowledge will be penalized by gradually shrinking them to zero during model training rather than straight zeroed out, giving the model some freedom to recover potentially important missing gene module-gene relationships.

Since the loss function with the L1 regularization term makes it a non-differentiable function, the proximal gradient algorithm\cite{Parikh2014} is employed to enable the model to be optimized during model training. The proximal gradient descent was used after the standard gradient descent to update decoder weights\cite{Seninge2021}, which is formulated as:
\begin{align}
    w^{(t+1)} &= w^t - G \;\;\;\;\;\; \to \textnormal{for updating weights of annotated connections}\label{eqn:11} \\
    w^{(t+1)} &= w^t - G - \lambda\alpha|w^t| \;\;\;\;\;\; \to \textnormal{for updating weights of unannotated connections}\label{eqn:22}
\end{align}
where $w^{(t+1)}$ denotes an updated weight, $w^t$ denotes a weight before updated and $G$ represents the standard gradient descent, which describes the updating of weights of annotated relationships between GMVs and feature genes in Eq.(\ref{eqn:11}). For those weights of unannotated relationships, besides the standard gradient descent, the weights are penalized by the proximal gradient descent $\lambda\alpha|w^t|$ where $\alpha$ is the learning rate, described in Eq.(\ref{eqn:22}). The product of $\lambda$ and $\alpha$ in the proximal gradient descent procedure is the hyperparameter to control the behavior of the regularized decoder, which can be intuitively seen as the degree of imposing L1 regularization on weights in the decoder. Basically, $\alpha$ is usually a fixed number, so $\lambda$ is the hyperparameter of interest.

\section{Bayesian differential activity analysis}\label{methods:bf}
The differences in activities of TFs or pathways between two groups of cells can always provide valuable biological insights. To this end, we employed the differential GMV activity analysis procedure proposed by Seninge et al. (2021), which is inspired by the Bayesian differential gene expression procedure introduced in Lopez el al. (2018). For simplicity, we follow the same notation as Seninge et al. (2021). For each GMV $k$ and pair of cells ($x_a, x_b$) with inferred GMV activities ($z_a, z_b$) and their group IDs ($s_a, s_b$) (e.g., two different cell types or cells treated under two different conditions), the two mutually exclusive hypotheses can be formulated as:
\begin{equation}
    \mathcal{H}^k_0 := E_s[z^k_a] > E_s[z^k_b]\;\;\textnormal{vs.}\;\;\mathcal{H}^k_1 := E_s[z^k_a] \leq E_s[z^k_b]
\end{equation}
where the expectation $E$ represents the empirical frequency. The hypotheses can be simply interpreted as whether a cell has a higher mean of a certain GMV activity than another. Finally, the more probable hypothesis can be determined by a single numeric, a log-Bayes factor\cite{Kass1995,Held2018} (BF), defined as:
\begin{equation}
    K = \textnormal{log}_e \frac{p(\mathcal{H}^k_0\,|\,x_a, x_b)}{p(\mathcal{H}^k_1\,|\,x_a,x_b)}
\end{equation}
The sign of $K$ indicates which hypothesis is more likely and the magnitude of $K$ reveals the significance level. Since the posterior distribution over each GMV can be approximated via the variational distribution (i.e., $q(Z\,|\,X)$, the encoding part of the model), the probability of each hypothesis can then be approximated by
\begin{equation}
    p(\mathcal{H}^k_0\,|\,x_a, x_b) \approx \sum_{s}p(s)\int\limits_{z_a}\int\limits_{z_b}p(z^k_a > z^k_b)\,dq(z^k_a\,|\,x_a)\,dq(z^k_b\,|\,x_b)
\end{equation}
where $p(s)$ denotes the relative abundance of cells in a group $s$ and the integrals can be computed using naive Monte Carlo\cite{math2021} due to the low dimensionality of all measures.

Sticking to the same assumption made in Lopez el al. (2018) and Seninge et al. (2021) that all cells are independent, we can compute the average Bayes factor across a large set of cell pairs where cells in each cell pair are randomly sampled and can be repeatedly taken from the corresponding cell groups (i.e., permutations), which brings about more comprehensive cell comparisons. The average Bayes factor can tell us if a GMV is more active at a higher frequency in one group or the other. In this work, we also considered GMVs to be significantly differentially activated when the absolute value of $K$ is greater than 3 (equivalent to a BF $\approx$ 20)\cite{Seninge2021,Kass1995,Lopez2018}.

\section{Datasets}\label{methods:data}
\subsection{Kang et al. dataset}
The PBMCs dataset\cite{Kang2018} consists of two groups of blood cells: Control cells and cells stimulated with interferon-$\beta$. The cell type annotation and the data preprocessing was conducted using Scanpy package\cite{Wolf2018} in Python and described in the VEGA paper\cite{Seninge2021}. The final dataset includes 16893 cells (8007 control cells and 8886 stimulated cells) with the top 6998 highly variable genes, which is downloadable at the GitHub repository\footnote{\label{footnote:reproducibility}\url{https://github.com/LucasESBS/vega-reproducibility}} provided by VEGA authors. The PBMCs dataset was used in this work to reproduce the results in the VEGA paper and investigate the stability of model training. The reproducibility code can also be downloaded at the same GitHub repository\textsuperscript{\ref{footnote:reproducibility}}.

\subsection{Jansky et al. datasets}
Two datasets from Jansky et al. (2021) were used in this work: (1) the human adrenal medulla dataset and (2) the human neuroblastoma dataset. The cell type annotation and the data preprocessing was performed using Seurat R package\cite{Stuart2019,Hao2021} and described in the paper\cite{Jansky2021}. The adrenal medulla dataset consists of human healthy cells spanning several different embryonic and fetal developmental time points, including SCPs, chromaffin cells, neuroblasts and the other transient populations which are termed bridge and connecting progenitor cells. The final adrenal medulla dataset includes 9387 cells with the whole 28422 genes or the top 2000 highly variable genes, which can be downloaded at the link\footnote{\url{https://adrenal.kitz-heidelberg.de/developmental_programs_NB_viz/}}. The human neuroblastoma dataset consists of mostly tumor cells and a small number of normal cells from 22 neuroblastoma samples, containing 104881 cells with 28312 genes. Even though the data integration was performed using Seurat R package, the combined neuroblastoma data still suffers from severe batch effects (without using Harmony\cite{Korsunsky2019}). Therefore, this dataset is favorable in our work for testing the function of batch correction of the model. Note that the designed model takes AnnData (annotated data) as input based on anndata Python package\cite{Virshup2021}, so sceasy R package\cite{Kiselev2020} was used to convert a Seurat object to AnnData which is in h5ad format.

\section{Prior biological abstractions for guiding decoder wiring}\label{methods:prior}
Prior biological abstractions which is composed of gene modules (gene sets) can be used to guide the connections of the linear decoder, which models the GMVs in the latent space as the certain biological entities\cite{Seninge2021}. In this work, the Reactome collection of pathways and processes\cite{Jassal2020} (674 gene sets) was employed as prior knowledge for reproducing the analyses of the PBMCs dataset\cite{Kang2018} from the VEGA paper\cite{Seninge2021}, which can be downloaded at the GitHub repository\footnote{\url{https://github.com/LucasESBS/vega-reproducibility}}. The SCENIC\cite{Aibar2017} regulons inferred from the human adrenal medulla dataset (215 gene sets) and the human neuroblastoma dataset (67 gene sets) were taken from Jansky et al. (2021) for investigating the ability and the properties of VEGA and the behavior of the model using the L1 regularized decoder. The non-context-specific DoRothEA\cite{Garcia-Alonso2019} regulons (1333 gene sets) were used for investigating the inference capacity of the regularized decoder, which can be accessed through dorothea R package. Of note, since the tasks where the DoRothEA regulons were used were closely associated with the adrenal medulla SCENIC regulons, the DoRothEA regulons were filtered based on the adrenal medulla SCENIC regulons, resulting in the final 211 gene sets.

\section{Examination of model reproducibility}\label{methods:stability}
The core value of VEGA\cite{Seninge2021} is its interpretable latent space that can provide meaningful biological insights at the single-cell level. As a result, the reproducibility of the latent space is fairly important. The strategy we used to measure the VEGA reproducibility was training two individual VEGA models using the same set of hyperparameters on the same dataset and computing a Pearson correlation coefficient of each GMV in the latent space between these two trained models. The Pearson correlation coefficient was computed using SciPy package\cite{Virtanen2020} in Python (\texttt{scipy.stats.pearsonr}).

\section{Randomization of \textit{a priori} defined gene sets}\label{methods:randomization}
It has been validated that using prior biological knowledge to guide the decoder wiring can model the latent variables as the interpretable GMVs\cite{Seninge2021,Rybakov2020,Lotfollahi2022}. To study the importance of correct prior knowledge concerning the model interpretability, we randomized predefined gene sets on varied levels. According to the degree of randomization, e.g., 50\%, half genes from every predefined gene set were extracted and put into a gene space that contained all extracted genes. Next, the certain number of genes which did not overlap with the earlier extracted genes were taken from the gene space to refill each of the gene sets. Of note, the number of extracted genes was determined by the size of a specific gene set multiplied by the randomization level and rounded, so gene sets with small sizes on the low randomization level may acquire zero gene to be extracted. In this scenario, the number of extracted gene was set to 1.

\section{Recovery plot for studying regularized decoder}\label{methods:recovery}
To study the behavior of the L1 regularized linear decoder, we made a recovery plot\cite{Quintero2020} on the weights of a specific GMV to each gene reconstruction in the decoder. The weights were ranked from high to low and represented by the rankings in the x-axis and the frequency increased 1 in the y-axis when the corresponding reconstructed gene is annotated in the prior gene module. The area under the curve (AUC) was then computed using SciPy package\cite{Virtanen2020} in Python (\texttt{scipy.integrate.simps}) and scaled by the reciprocal of the maximum square area (the number of genes in the feature list multiplied by the number of the intersection of the certain gene module and the feature list) to evaluate the degree of the prior used by the model. In general, the weights of the GMV to the gene reconstructions where genes are annotated in the prior gene module should be relatively high (i.e., the reconstructed genes annotated in the prior gene module averagely have the higher ranks), resulting in the higher scaled AUC score. Of note, zero-valued weights were randomly ranked.

\section{Implementation of UMAP for visualization}\label{methods:umap}
To visualize datasets and model embeddings, the UMAP algorithm\cite{McInnes2020} was used via Scanpy Python package\cite{Wolf2018} where \texttt{scanpy.pp.neighbors} was first employed to compute a neighborhood graph of data and then \texttt{scanpy.tl.umap} was run on the neighborhood graph for dimensionality reduction. All parameters were set as the default. For the processed data from Jansky et al. (2021), the UMAP embeddings of gene expression spaces had been computed via Seurat R package\cite{Stuart2019,Hao2021}. Hence, we directly used \texttt{scanpy.pl.umap} to visualize the embeddings.
