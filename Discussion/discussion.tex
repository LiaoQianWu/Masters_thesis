\chapter{Discussion}
In this work, we investigated the ability and various properties of two kinds of interpretable VAEs which are used to analyze single-cell transcriptomes, introduced in Seninge et al. (2021) and Rybakov et al. (2020) respectively. Firstly, VEGA\cite{Seninge2021} hard codes the wiring of its single-layer linear decoder through a binary mask that is subject to prior biological knowledge. By limiting the connection of latent variables to certain sets of genes in the output layer, the latent variables can be modeled as a list of gene modules depending on provided prior knowledge. This operation makes VEGA extremely flexible to take advantage of diverse sorts of known biology as prior knowledge, such as pathways, GRNs and cell type marker genes, as long as the prior consists of biologically meaningful gene sets, which enables us to interpret single-cell transcriptomes from many other different viewpoints. From another perspective, VEGA provides an efficient way of inferring the activity of gene modules from transcriptomic data at the single-cell level. To be more concrete, in our work, we analyzed the human adrenal medulla dataset\cite{Jansky2021} using the SCENIC regulons\cite{Jansky2021} inferred from the same dataset as the prior to infer the TF activities of individual adrenal medullary cells. Note that VEGA can be seen as an alternative method to the AUCell algorithm in the SCENIC workflow\cite{Aibar2017}, which aims at scoring \textit{a priori} gene sets to identify active TF activities in every single cell. Next, the interpretable VEGA embedding containing inferred gene module activities of all cells can be used for many downstream studies, such as cell type identities, cellular states of different cell populations and so on. Since each GMV is encoded into the posterior distribution, it also provides a natural way of conducting various types of hypothesis testing, e.g., GMV differential activity analysis\cite{Seninge2021} used in this work. Our analysis results demonstrate the potential of VEGA in terms of inferring reasonable GMV activities from single-cell transcriptomic data when prior knowledge is provided to guide the decoder wiring (see Section \ref{sec:sparse_adm} in Results). Apart from the TFs of interest based on the inferred TF activity result obtained using the AUCell algorithm (Fig.\ref{fig:sparse_adm_scenic}C), VEGA also captured some other significantly differential TF activities between different cell types (Appx.\ref{fig:sparse_adm_scenic_appx}D), which is worth further investigation. However, we did not find any supporting biological evidence for them from previous studies. Therefore, whether these significantly differential TF activities are biologically meaningful or false positives remains unclear.

For the model reproducibility which is important to the core value of VEGA, interpretability, we observed that using a dropout layer in the latent space improves the overall model training stability (see Section \ref{sec:reproducibility} in Results). Besides, using a dropout layer in the latent space also has been proved to be an effective way of preserving the redundancy between similar GMVs (i.e., prior gene modules whose gene sets are overlapped with each other)\cite{Seninge2021}. However, there were still a number of unrelated or even anticorrelated GMVs between two individual trained models, and we found that a subgroup of GMVs whose decoder wiring is highly similar tend to have low correlations. Note that the PBMCs dataset\cite{Kang2018} and the Reactome pathways\cite{Jassal2020} are discussed in this part. For example, we observed that the GMVs representing the Notch signaling pathways (NOTCH2, NOTCH3 and NOTCH4) whose gene sets are 92\% overlapped with each other had the poor reproducibility despite a dropout layer used in the latent space (Table \ref{table:notch}). The reason may be that the learned GMV-specific information is randomly distributed over these highly similar GMVs due to the overlapping decoder connections, so the information these GMVs hold will be more variable from training to training, which also implies the loss of the GMV interpretability. Yet, highly overlapping prior gene sets is not always the cause because we also observed subgroups of highly similar GMVs had the favorable reproducibility. Therefore, further work on discerning causes of poor reproducibility and finding a way to adress this issue for generalizing the model to possibly all gene modules is needed.

\begin{table}[h!]
    \begin{center}
        \captionsetup{width=.73\textwidth}
        \caption{\small{\textbf{Example of poor reproducibility of highly overlapping gene modules} | $z\_dropout$ indicates the dropout rate of a dropout layer used in the latent space. The numeric values indicate the PCC of a certain gene module between two individual trained models.}}
        \label{table:notch}
        \begin{tabular}{|c|c|c|c|}
        \hline
        & \textbf{\small{\boldmath{$z\_dropout=0$}}} & \textbf{\small{\boldmath{$z\_dropout=0.3$}}} & \textbf{\small{\boldmath{$z\_dropout=0.5$}}}\\
        \hline
        \textbf{\small{NOTCH2}} & \small{0.006} & \small{0.308} & \small{0.238}\\
        \hline
        \textbf{\small{NOTCH3}} & \small{0.013} & \small{0.320} & \small{0.189}\\
        \hline
        \textbf{\small{NOTCH4}} & \small{0.018} & \small{0.333} & \small{0.219}\\
        \hline
        \end{tabular}
    \end{center}
\end{table}

Secondly, one big limitation of VEGA lies in its hard-coded decoder which leaves no room for further correcting or expanding existing biological knowledge bases\cite{Seninge2021}. To this end, instead of hard coding, the L1 regularization technique can be selectively employed on weights in the single-layer linear decoder through a binary mask based on the prior. By penalizing the weight of decoder connections that are not included in the prior (i.e., gradually shrinking the weight of unannotated relationships between GMVs and genes to zero), the model has a chance to preserve potentially important unannotated genes for a certain gene module and in the meantime, can exclude computationally unassociated genes. Note that the human adrenal medulla dataset\cite{Jansky2021} and the SCENIC adrenal medulla regulons\cite{Jansky2021} are discussed in this part. Our experimental results indicate that the model using the regularized decoder had the ability to recover the missing target genes and expand the SCENIC adrenal medulla regulons (see Section \ref{sec:L1_adm_removed} in Results). However, for non-biologically meaningful genes, the model could exclude them in certain conditions but there was no clear pattern of how the regularized decoder handle those artificially added genes (see Section \ref{sec:L1_adm_added} in Results). Besides, we observed that genes with a high average expression level are more likely to have high weight values. Theoretically, the reason might be that genes having a high expression level usually indicate they also have high variance, so they need higher weights to take care of the reconstruction of more variable gene expression levels. Generally, we regard the weight of decoder connections as the strength of relationships between GMVs and genes, yet whether the decoder weights are biologically meaningful or just computationally meaningful needs further investigation.

Moreover, we used the non-context-specific DoRothEA regulons\cite{Garcia-Alonso2019} as the prior in an attempt to infer more dataset-specific regulons (see Section \ref{sec:L1_dorothea}). Our experimental results show that the model using the regularized decoder can potentially infer more dataset-specific GRNs from the DoRothEA regulons. However, we also observed that the inference of dataset-specific regulons cannot be generalized to a majority of TFs (Appx. \ref{fig:L1_dorothea_scenic_heatmap}). According to our current knowledge, apart from the value of $\lambda\alpha$, there are many other factors in control of the behavior of the regularized decoder, such as the combination of target gene sets of each TF (i.e., the association of each gene with GMVs). Hence, further work on deciphering the decoder behavior in more detail is needed so that the generalization of the regularized decoder can be improved. To this end, it is an interesting question whether the model using the regularized decoder can be an alternative method to SCENIC because, in terms of the runtime, the inference of putative regulons and TF activities in individual cells using the model with the regularized decoder is much faster than SCENIC. Even so, as frequently mentioned above, putative regulons inferred by the model using the regularized decoder may contain many false positives (i.e., non-biologically meaningful target genes) due to a data-driven fashion. Functionally, the model using the regularized decoder can be roughly seen as the SCENIC workflow without the second step, motif enrichment analysis, which enables inferred regulons to only keep direct-binding target genes. Therefore, further work can be either proving putative regulons inferred by the model using the regularized decoder are mostly biologically meaningful or developing the model that can incorporate more prior information (e.g., motifs) to exclude false positives.

To sum up, VAEs either using the hard-coded decoder or using the regularized decoder effectively enhance the interpretability of the latent space, which can present the inferred activity of diverse sorts of gene modules from scRNA-seq data. These two different methods to boost the model interpretability can be complementary to each other due to their unique attributes. The VAEs using the hard-coded decoder can faithfully interpret the learned variation between individual cells from different points of view according to provided prior knowledge but are lack of flexibility in terms of exploring unknown or context-specific biological knowledge. Hence, it is a more desirable method when the prior is well-studied or fully context-specific. On the other hand, the VAEs using the regularized decoder enable the model to potentially correct or expand existing biological knowledge but the new-found may be false positives owing to a data-driven fashion.
